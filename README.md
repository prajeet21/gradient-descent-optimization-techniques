# gradient-descent-optimization-techniques

Implemented a 3 layer fully connected neural network for the Fashion MNIST dataset with 500 and 100 nodes in the two hidden layers respectively and relu as the activation function. 

The aim of this project was to evaluate various gradient descent optimization techniques: 
* Polyak’s classical momentum
* Nesterov’s Accelerated Gradient
* RMSProp
* ADAM
